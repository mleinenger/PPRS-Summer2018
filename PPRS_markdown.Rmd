---
title: "PPRS"
author: "Mallorie Leinenger"
date: "7/16/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Documents/PPRS")
```

# Phonological and Predictive Reading Strategies 

## Overview
These data come from an eye tracking experiment investigating phonological coding and predictability effects during silent reading. To measure the use of phonological coding, participants read 180 neutral sentences that contained a contextually appropriate (but not predictable) target word. We used the gaze-contingent boundary paradigm (Rayner, 1975) to manipulate the preview information that readers had prior to directly fixating the target word. Readers either had an *identical* preview of the target (*beach*-*beach*), a *phonologically related* preview (*beech*-*beach*), or a control preview that shared the same number of letters with the correct target as the phonologically related preview, but did not fully overlap on phonology (*bench*-*beach*). We recorded readers eye movements as they read the sentences and compared reading time on the target word as a function of the type of preview to investigate the use of phonological codes. Following Leinenger (2018), individual participant survival analyses were computed on the first fixation duration data for the phonologically related and control conditions to determine the divergence point estimate for each participant. This estimate was taken to represent the earliest observable influence of phonology on the eye movement record and therefore as an index of how early participants were generating and using phonological codes, relative to one another.

We also included 62 sentences with a manipulation of contextual constraint to investigate reliance on predictive strategies. Participants read sentences which either rendered a target word highly predictable or not predictable, and we compared reading time as a function of the constraint condition. Larger differences between the predictable and unpredictable conditions (whereby the predictable targets were read faster than the unpredictable) were taken as reflecting a greater reliance on context for generating predictions about upcoming words. Example stimuli appear below:

- She moved from the country to the large **city** to find a better job. (Cloze = .9, predictable)
- We took a walk in the quiet **city** before we drove back home. (Cloze = 0, unpredictable)

Ultimately, participants scores on a battery of offline language assessments (verbal fluency, ) were related to their use of phonological coding and predictive strategies, to determine what specific language skills (or language skill profiles) were associated with different online reading strategies.

```{r load packages, echo = FALSE, message=FALSE, warning=FALSE}
## load required packages
library(tidyverse)
library(lme4)
library(psych)
library(plotrix)
library(reshape2)
library(RTsurvival)
library(survival)
```

## Examining the use of Phonological Codes

```{r read phonological data, echo = TRUE, message = FALSE, warning = FALSE}
# read in the data
d <- read.csv("PPRS_phon.csv")

# create new "Preview" variable and recode variables into factors
d$Preview <- as.factor(ifelse(d$cond==1,"Identical", ifelse(d$cond==2, "Phonologically Related", "Control")))
d$previewtype<-as.factor(ifelse(d$cond == 1, 2, ifelse(d$cond == 2, 1, 3)))
d$skp<-ifelse(d$skp==100,0,1)
d$subj<-as.factor(d$subj)
d$cond<-as.factor(d$cond)
d$item<-as.factor(d$item)
d$seq<-as.factor(d$seq)

# ffd = first fixation duration, sfd = single fixation duration, gzd = gazd duration, gpt = go-past time, tvt = total time, skp = skipping probability, rgo = regression out probability, rgi = regression in probability
str(d)
head(d)

# compute basic summary statistics
byCond <- group_by(d, Preview)
stats.m<-summarize_if(byCond,is.numeric,funs(mean), na.rm=TRUE)
stats.se<-summarize_if(byCond,is.numeric,funs(std.error), na.rm=TRUE)
stats.m
stats.se

# create a figure that plots duration measures

d$Preview<- factor(d$Preview,levels(d$Preview)[c(2,3,1)])
dat.SCA<-melt(data=d, id = c("subj","item","Preview"), measure = c("ffd","sfd","gzd","gpt","tvt"))
p.meanse.SCA<-ggplot(data = subset(dat.SCA, variable %in% c("ffd","sfd","gzd","gpt","tvt")), aes(y = value, x= Preview, shape=Preview, color=Preview)) + 
  stat_summary(fun.data = "mean_se") + stat_summary(fun.y = mean, geom = "point") + facet_grid(.~variable) +
  labs(y = "Reading Time (ms)", x = "", shape="Preview Condition", color="Preview Condition") + theme_grey(base_size=16) + scale_y_continuous(breaks=seq(0,500,20)) + 
  theme(axis.text.x = element_text(colour="grey4", size=0), axis.text.y = element_text(colour = "grey4", size=20))
p.meanse.SCA + scale_colour_manual(values=c("aquamarine4","aquamarine3","coral3")) + scale_shape_manual(values=c(1,0,2)) 

# create a figure that plots probability measures

dat.SCA<-melt(data=d, id = c("subj","item","Preview"), measure = c("skp","rgo","rgi"))
p.meanse.SCA<-ggplot(data = subset(dat.SCA, variable %in% c("skp","rgo","rgi")), aes(y = value, x= Preview, shape=Preview, color=Preview)) + 
  stat_summary(fun.data = "mean_se") + stat_summary(fun.y = mean, geom = "point") + facet_grid(.~variable) +
  labs(y = "Probability (%)", x = "", shape="Preview Condition", color="Preview Condition") + theme_grey(base_size=16) + scale_y_continuous(breaks=seq(0,1,.02)) + 
  theme(axis.text.x = element_text(colour="grey4", size=0), axis.text.y = element_text(colour = "grey4", size=20))
p.meanse.SCA + scale_colour_manual(values=c("aquamarine4","aquamarine3","coral3")) + scale_shape_manual(values=c(1,0,2)) 
```

## Running linear mixed effect regression models to determine significant mean differences
For each model, the phonologically related condition is represented by the intercept and treatment coded contrasts test for significant differences between the phonologically

```{r models for phonological data, echo = TRUE, message = FALSE, warning = FALSE}
# first fixation duration

#lm.ffd<-lmer(ffd ~ previewtype + (previewtype|subj) + (previewtype|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))) failed to converge

#lm.ffd<-lmer(ffd ~ previewtype + (1|subj) + (0 + previewtype|subj) + (previewtype|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))) failed to converge

#lm.ffd<-lmer(ffd ~ previewtype + (1|subj) + (0 + previewtype|subj) + (1|item) + (0 + previewtype|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))) failed to converge

lm.ffd<-lmer(ffd ~ previewtype + (previewtype|subj) + (1|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(lm.ffd)

# single fixation duration

lm.sfd<-lmer(sfd ~ previewtype + (previewtype|subj) + (previewtype|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(lm.sfd)

# gaze duration

#lm.gzd<-lmer(gzd ~ previewtype + (previewtype|subj) + (previewtype|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))) failed to converge

#lm.gzd<-lmer(gzd ~ previewtype + (1|subj) + (0 + previewtype|subj) + (previewtype|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))) failed to converge

#lm.gzd<-lmer(gzd ~ previewtype + (1|subj) + (0 + previewtype|subj) + (1|item) + (0 + previewtype|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))) failed to converge

lm.gzd<-lmer(gzd ~ previewtype + (previewtype|subj) + (1|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(lm.gzd)

# go-past time

lm.gpt<-lmer(gpt ~ previewtype + (previewtype|subj) + (previewtype|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(lm.gpt)

# total time

#lm.tvt<-lmer(tvt ~ previewtype + (previewtype|subj) + (previewtype|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))) failed to converge

#lm.tvt<-lmer(tvt ~ previewtype + (1|subj) + (0 + previewtype|subj) + (previewtype|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))) failed to converge

#lm.tvt<-lmer(tvt ~ previewtype + (1|subj) + (0 + previewtype|subj) + (1|item) + (0 + previewtype|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))) failed to converge

lm.tvt<-lmer(tvt ~ previewtype + (previewtype|subj) + (1|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(lm.tvt)

# skipping

glm.skp <- glmer(skp ~ previewtype + (previewtype|subj) + (previewtype|item), data=d, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(glm.skp)

# regression-out

glm.rgo <- glmer(rgo ~ previewtype + (previewtype|subj) + (previewtype|item), data=d, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(glm.rgo)

# regression-in

#glm.rgi <- glmer(rgi ~ previewtype + (previewtype|subj) + (previewtype|item), data=d, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000))) failed to converge

glm.rgi <- glmer(rgi ~ previewtype + (1|subj) + (0 + previewtype|subj) + (previewtype|item), data=d, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(glm.rgi)
```

## Computing Divergence Point Estimates using IP-DPA Survival Analysis Technique (Reingold & Sheridan, 2014)

```{r survdata setup, echo = TRUE, message=FALSE, warning=FALSE}
tmp<-select(d,subj,ffd,cond) %>% arrange(cond) %>%
  rename(duration=ffd) %>% rename(subject=subj) %>% rename(condition=cond)
tmp$condition <- as.numeric(tmp$condition)
survdata<- as.data.frame(tmp %>% as_tibble() %>% mutate(condition = condition-1)) %>%
  filter(condition != 0, !is.na(duration))
survdata$condition<- as.factor(survdata$condition)
str(survdata)
```

We have 31 participants who each have between 36 and 105 data points:

```{r survdata summary Exp 1, echo = TRUE, message=FALSE, warning=FALSE} 
n.per.sbj <- table(survdata$subject)
length(n.per.sbj)
range(n.per.sbj)
```

We can now use these data to generate divergence point estimates (DPE) for each participant:

```{r ip.DPA Exp 1, echo = TRUE, message=FALSE, warning=FALSE}
ip.dpa <- DPA.ip(survdata$subject, survdata$duration, survdata$condition, quiet = TRUE)
dpe <- as.data.frame(ip.dpa$dp_matrix)
# critical columns in output
# 'dpcount' = the number of iterations (out of 1000) on which a DPE was obtained
#' median_dp_duration' = median of the DPEs obtained on each iteration
str(dpe)
dpe$subject[dpe$dpcount<500]
```

Doing so reveals that the DPE for 6 participants were unreliable (i.e., a DP was found on fewer than half of the iterations). Removing those participants reveals a mean DPE of ~192 across the remaining participants (the value moves around ever so slightly each time the bootstrap re-sampling procedure runs). This tells us that on average, phonological coding was influencing behavior by as early as 192 ms after fixation on the target word began.

```{r DPE estimate Exp 1, echo = TRUE, message=FALSE, warning=FALSE}
dpe.rel <- filter(dpe, dpcount >= 500)
summarize(dpe.rel, mean.dpe = mean(median_duration, na.rm=TRUE))

DP<-mean(dpe.rel$median_duration)
ci.lower<-mean(dpe.rel$ci.lower)
ci.upper<-mean(dpe.rel$ci.upper)
```

Finally, we can represent this visually by examining the survival curves created using the ggsurv function.

```{r ggsurv function, echo = FALSE, message=FALSE, warning=FALSE}
ggsurv <- function(s, CI = 'def', plot.cens = T, surv.col = c("coral3","aquamarine3"),
                   cens.col = 'red', lty.est = c(2,1), lty.ci = 2,
                   cens.shape = 3, back.white = F, xlab = 'Time',
                   ylab = 'Survival', main = ''){
  
  library(ggplot2)
  strata <- ifelse(is.null(s$strata) ==T, 1, length(s$strata))
  stopifnot(length(surv.col) == 1 | length(surv.col) == strata)
  stopifnot(length(lty.est) == 1 | length(lty.est) == strata)
  
  ggsurv.s <- function(s, CI = 'def', plot.cens = T, surv.col = c("aquamarine4","aquamarine3","coral3"),
                       cens.col = 'red', lty.est = 1, lty.ci = 2,
                       cens.shape = 3, back.white = F, xlab = 'Time',
                       ylab = 'Survival', main = ''){
    
    dat <- data.frame(time = c(0, s$time),
                      surv = c(1, s$surv),
                      up = c(1, s$upper),
                      low = c(1, s$lower),
                      cens = c(0, s$n.censor))
    dat.cens <- subset(dat, cens != 0)
    
    col <- ifelse(surv.col == 'gg.def', 'black', surv.col)
    
    pl <- ggplot(dat, aes(x = time, y = surv)) +
      xlab(xlab) + ylab(ylab) + ggtitle(main) +
      geom_step(col = col, lty = lty.est)
    
    pl <- if(CI == T | CI == 'def') {
      pl + geom_step(aes(y = up), color = col, lty = lty.ci) +
        geom_step(aes(y = low), color = col, lty = lty.ci)
    } else (pl)
    
    pl <- if(plot.cens == T & length(dat.cens) > 0){
      pl + geom_point(data = dat.cens, aes(y = surv), shape = cens.shape,
                      col = cens.col)
    } else if (plot.cens == T & length(dat.cens) == 0){
      stop ('There are no censored observations')
    } else(pl)
    
    pl <- if(back.white == T) {pl + theme_bw()
    } else (pl)
    pl
  }
  
  ggsurv.m <- function(s, CI = 'def', plot.cens = T, surv.col = c("aquamarine4","aquamarine3","coral3"),
                       cens.col = 'red', lty.est = 1, lty.ci = 2,
                       cens.shape = 3, back.white = F, xlab = 'Time',
                       ylab = 'Survival', main = '') {
    n <- s$strata
    
    groups <- factor(unlist(strsplit(names
                                     (s$strata), '='))[seq(2, 2*strata, by = 2)])
    gr.name <-  unlist(strsplit(names(s$strata), '='))[1]
    gr.df <- vector('list', strata)
    ind <- vector('list', strata)
    n.ind <- c(0,n); n.ind <- cumsum(n.ind)
    for(i in 1:strata) ind[[i]] <- (n.ind[i]+1):n.ind[i+1]
    
    for(i in 1:strata){
      gr.df[[i]] <- data.frame(
        time = c(0, s$time[ ind[[i]] ]),
        surv = c(1, s$surv[ ind[[i]] ]),
        up = c(1, s$upper[ ind[[i]] ]),
        low = c(1, s$lower[ ind[[i]] ]),
        cens = c(0, s$n.censor[ ind[[i]] ]),
        group = rep(groups[i], n[i] + 1))
    }
    
    dat <- do.call(rbind, gr.df)
    dat.cens <- subset(dat, cens != 0)
    
    pl <- ggplot(dat, aes(x = time, y = surv, group = group)) +
      xlab(xlab) + ylab(ylab) + ggtitle(main) +
      geom_step(aes(col = group, lty = group))
    
    col <- if(length(surv.col == 1)){
      scale_colour_manual(name = gr.name, values = rep(surv.col, strata))
    } else{
      scale_colour_manual(name = gr.name, values = surv.col)
    }
    
    pl <- if(surv.col[1] != 'gg.def'){
      pl + col
    } else {pl + scale_colour_discrete(name = gr.name)}
    
    line <- if(length(lty.est) == 1){
      scale_linetype_manual(name = gr.name, values = rep(lty.est, strata))
    } else {scale_linetype_manual(name = gr.name, values = lty.est)}
    
    pl <- pl + line
    
    pl <- if(CI == T) {
      if(length(surv.col) > 1 && length(lty.est) > 1){
        stop('Either surv.col or lty.est should be of length 1 in order
             to plot 95% CI with multiple strata')
      }else if((length(surv.col) > 1 | surv.col == c("aquamarine4","aquamarine3","coral3"))[1]){
        pl + geom_step(aes(y = up, color = group), lty = lty.ci) +
          geom_step(aes(y = low, color = group), lty = lty.ci)
      } else{pl +  geom_step(aes(y = up, lty = group), col = surv.col) +
          geom_step(aes(y = low,lty = group), col = surv.col)}
    } else {pl}
    
    
    pl <- if(plot.cens == T & length(dat.cens) > 0){
      pl + geom_point(data = dat.cens, aes(y = surv), shape = cens.shape,
                      col = cens.col)
    } else if (plot.cens == T & length(dat.cens) == 0){
      stop ('There are no censored observations')
    } else(pl)
    
    pl <- if(back.white == T) {pl + theme_bw()
    } else (pl)
    pl
  }
  pl <- if(strata == 1) {ggsurv.s(s, CI , plot.cens, surv.col ,
                                  cens.col, lty.est, lty.ci,
                                  cens.shape, back.white, xlab,
                                  ylab, main)
  } else {ggsurv.m(s, CI, plot.cens, surv.col ,
                   cens.col, lty.est, lty.ci,
                   cens.shape, back.white, xlab,
                   ylab, main)}
  pl
}

```
```{r survival figure Exp 1, echo = FALSE, message=FALSE, warning=FALSE}
d$survdat<-as.integer(ifelse(d$Preview=="Identical",NA,d$ffd))
tmp2 <- filter(d, !subj %in% c(1, 15, 16, 17, 19, 31))
ffd.surv <- survfit(Surv(survdat) ~ Preview, data=tmp2)
pl2<-ggsurv(s=ffd.surv)

#my.label1 = bquote("Divergence Point " ~ .(format(DP, digits=3)) ~ "ms")
#my.label2 = bquote("95% CI: " ~ .(format(ci.lower, digits=3)) ~ "-" ~ .(format(ci.upper, digits=3)) ~ "ms")
  
pl2 + geom_vline(xintercept = DP, linetype = "dotted") + 
  annotate("rect", xmin=ci.lower, xmax=ci.upper, ymin=0, ymax=1, alpha = .2) + 
  annotate("text", x = 475, y = 0.75, label = "Mean Divergence Point = 192ms", size = 3.5) +
  annotate("text", x = 475, y = 0.7, label = "Range: 95.5-319.5ms", size = 3.5) + 
  theme(axis.text.x = element_text(colour="grey4", size=16), axis.text.y = element_text(colour = "grey4", size=16)) + 
  labs(y = "Survival", x = "Time") + theme_grey(base_size=16)
```

## Examining the use of Context and Predictive Strategies

```{r read predictability data, echo = TRUE, message = FALSE, warning = FALSE}
# read in the data
d <- read.csv("PPRS_pred.csv")

# create new "Constraint" variable and recode variables into factors
d$Constraint <- as.factor(ifelse(d$cond==4,"High", "Low"))
d$skp<-ifelse(d$skp==100,0,1)
d$subj<-as.factor(d$subj)
d$cond<-as.factor(d$cond)
d$item<-as.factor(d$item)
d$seq<-as.factor(d$seq)

str(d)
head(d)

# compute basic summary statistics
byCond <- group_by(d, Constraint)
stats.m<-summarize_if(byCond,is.numeric,funs(mean), na.rm=TRUE)
stats.se<-summarize_if(byCond,is.numeric,funs(std.error), na.rm=TRUE)
stats.m
stats.se

# create a figure that plots duration measures

dat.SCA<-melt(data=d, id = c("subj","item","Constraint"), measure = c("ffd","sfd","gzd","gpt","tvt"))
p.meanse.SCA<-ggplot(data = subset(dat.SCA, variable %in% c("ffd","sfd","gzd","gpt","tvt")), aes(y = value, x= Constraint, shape=Constraint, color=Constraint)) + 
  stat_summary(fun.data = "mean_se") + stat_summary(fun.y = mean, geom = "point") + facet_grid(.~variable) +
  labs(y = "Reading Time (ms)", x = "", shape="Constraint", color="Constraint") + theme_grey(base_size=16) + scale_y_continuous(breaks=seq(0,500,20)) + 
  theme(axis.text.x = element_text(colour="grey4", size=0), axis.text.y = element_text(colour = "grey4", size=20))
p.meanse.SCA + scale_colour_manual(values=c("dodgerblue1","grey0")) + scale_shape_manual(values=c(1,0)) 

# create a figure that plots probability measures

dat.SCA<-melt(data=d, id = c("subj","item","Constraint"), measure = c("skp", "rgo", "rgi"))
p.meanse.SCA<-ggplot(data = subset(dat.SCA, variable %in% c("skp", "rgo", "rgi")), aes(y = value, x= Constraint, shape=Constraint, color=Constraint)) + 
  stat_summary(fun.data = "mean_se") + stat_summary(fun.y = mean, geom = "point") + facet_grid(.~variable) +
  labs(y = "Reading Time (ms)", x = "", shape="Constraint", color="Constraint") + theme_grey(base_size=16) + scale_y_continuous(breaks=seq(0,1,.02)) + 
  theme(axis.text.x = element_text(colour="grey4", size=0), axis.text.y = element_text(colour = "grey4", size=20))
p.meanse.SCA + scale_colour_manual(values=c("dodgerblue1","grey0")) + scale_shape_manual(values=c(1,0)) 
```

## Running linear mixed effect regression models to determine significant mean differences

```{r models for predictability data, echo = TRUE, message = FALSE, warning = FALSE}

# setting up contrasts (deviation coding)

contrasts(d$Constraint) <- rbind(-.5,.5)

# first fixation duration

lm.ffd<-lmer(ffd ~ Constraint + (Constraint|subj) + (Constraint|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(lm.ffd)

# single fixation duration

lm.sfd<-lmer(sfd ~ Constraint + (Constraint|subj) + (Constraint|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(lm.sfd)

# gaze duration

lm.gzd<-lmer(gzd ~ Constraint + (Constraint|subj) + (Constraint|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(lm.gzd)

# go-past time

lm.gpt<-lmer(gpt ~ Constraint + (Constraint|subj) + (Constraint|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(lm.gpt)

# total time

lm.tvt<-lmer(tvt ~ Constraint + (Constraint|subj) + (Constraint|item), data=d, control=lmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(lm.tvt)

# skipping

glm.skp <- glmer(skp ~ Constraint + (Constraint|subj) + (Constraint|item), data=d, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(glm.skp)

# regression-out

glm.rgo <- glmer(rgo ~ Constraint + (Constraint|subj) + (Constraint|item), data=d, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(glm.rgo)

# regression-in

glm.rgi <- glmer(rgi ~ Constraint + (Constraint|subj) + (Constraint|item), data=d, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(glm.rgi)
```

## Computing the effect of predictability for each participant using Survival Analyses

```{r predictability effect, echo = TRUE, message = FALSE, warning = FALSE}
tmp<-select(d,subj,ffd,cond) %>% arrange(cond) %>%
  rename(duration=ffd) %>% rename(subject=subj) %>% rename(condition=cond)
tmp$condition <- as.numeric(tmp$condition)
survdata<- as.data.frame(tmp %>% as_tibble() %>% filter(!is.na(duration)))
survdata$condition<- as.factor(survdata$condition)
str(survdata)
```

We have 31 participants who each have between 24 and 62 data points:

```{r survdata summary pred, echo = TRUE, message=FALSE, warning=FALSE} 
n.per.sbj <- table(survdata$subject)
length(n.per.sbj)
range(n.per.sbj)
```

We can now use these data to generate divergence point estimates (DPE) for each participant:

```{r ip.DPA pred, echo = TRUE, message=FALSE, warning=FALSE}
ip.dpa <- DPA.ip(survdata$subject, survdata$duration, survdata$condition, quiet = TRUE)
dpe <- as.data.frame(ip.dpa$dp_matrix)
# critical columns in output
# 'dpcount' = the number of iterations (out of 1000) on which a DPE was obtained
#' median_dp_duration' = median of the DPEs obtained on each iteration
str(dpe)
dpe$subject[dpe$dpcount<500]
```

Doing so reveals that the DPE for 1 participant was unreliable (i.e., a DP was found on fewer than half of the iterations). Removing that participant reveals a mean DPE of ~151 ms across the remaining participants (the value moves around ever so slightly each time the bootstrap re-sampling procedure runs). This tells us that on average, the constraint manipulation was influencing behavior by as early as 151 ms after fixation on the target word began.

```{r DPE estimate pred, echo = TRUE, message=FALSE, warning=FALSE}
dpe.rel <- filter(dpe, dpcount >= 500)
summarize(dpe.rel, mean.dpe = mean(median_duration, na.rm=TRUE))

DP<-mean(dpe.rel$median_duration)
ci.lower<-mean(dpe.rel$ci.lower)
ci.upper<-mean(dpe.rel$ci.upper)
```

And we can plot the survival figure for the entire group of participants.

```{r survival figure pred, echo = TRUE, message=FALSE, warning=FALSE}
d$survdat<-as.integer(d$ffd)
tmp2 <- filter(d, !subj %in% c(4))
ffd.surv <- survfit(Surv(survdat) ~ Constraint, data=tmp2)
pl2<-ggsurv(s=ffd.surv)

#my.label1 = bquote("Divergence Point " ~ .(format(DP, digits=3)) ~ "ms")
#my.label2 = bquote("95% CI: " ~ .(format(ci.lower, digits=3)) ~ "-" ~ .(format(ci.upper, digits=3)) ~ "ms")
  
pl2 + geom_vline(xintercept = DP, linetype = "dotted") + 
  annotate("rect", xmin=ci.lower, xmax=ci.upper, ymin=0, ymax=1, alpha = .2) + 
  annotate("text", x = 475, y = 0.75, label = "Mean Divergence Point = 151ms", size = 3.5) +
  annotate("text", x = 475, y = 0.7, label = "Range: 76-264.5ms", size = 3.5) + 
  theme(axis.text.x = element_text(colour="grey4", size=16), axis.text.y = element_text(colour = "grey4", size=16)) + 
  labs(y = "Survival", x = "Time") + theme_grey(base_size=16)
```

## Determining the size of the predictability effect for each participant.

We can compute each participant's mean gzd for the high and low constraint items and then subtract their gzd in the high constraint condition from their gzd in the low constraint condition to determine the size of each individual participant's predictability effect.

```{r size pred effect, echo = TRUE, message=FALSE, warning=FALSE}

tmp <- setNames(aggregate(d$gzd, by=list(d$subj,d$cond), FUN=mean, na.rm=TRUE), c("subject","condition","gzd"))
pred <- setNames(spread(tmp,condition,gzd), c("subject","HC","LC"))
pred$diff <- pred$LC-pred$HC
head(pred,31)
```

## Cluster Analysis to determine different reader profiles

Examining the relationship between reading strategies (e.g., the use of phonological coding to identify words or context to predict/constrain the possibilities for upcoming words) and underlying language skill profiles using cluster analysis

We used Ward's Method, which is a hierarchical, agglomerative clustering procedure, and considered 5 clustering solutions (2- to 6-cluster solutions). The input consisted of z-transformed data for the following measures for the 25 participants without missing cases.  
**Mean.GZD** - the mean gaze duration in the identical preview condition.  
**DPE.Phon** - the individual participant's divergence point (calculated from the survival analysis above).  
**Pred.effect.GZD** - the size of the predictability effect (in ms) calculated by subtracting a participant's mean gaze duration in the high constraint condition from their mean gaze duration in the low constraint condition.  
**Spelling** - participant's raw score on the Woodcock Johnson spelling assessment.  
**T.PDE.WPS** - participant's score on the TOWRE Phonemic Decoding Efficiency subtest (converted into a words per second measure).  
**T.SWE.WPS** - participant's score on the TOWRE Sight Word Efficiency subtest (converted into a words per second measure).  
**Vocab** - participant's raw score on the WASI II Vocabulary test.

*We did not include scores on the PIAT.R reading comprehension test when performing the cluster analysis due to reduced variability in scores. We also chose to use the size of the predictability effect in ms rather than the DPE for our measure of reliance on predictive strategies due to greater variability in scores.*

```{r cluster analysis set up, echo = TRUE, message=FALSE, warning=FALSE}
# 'clusterdata' is a data frame with one row per subject and one column
# per measure (as numeric, z-transformed values) that you want the cluster analysis to take into account.
# Remove all columns that you do not want the cluster analysis to use and any rows with missing data.

t <- read.csv("PPRS_Assessment_EM.csv")
cd <- data.frame(matrix(NA, nrow=31, ncol=0))
cd$sub <- as.factor(t$ID)
cd$Verbal.Fluency <- as.numeric(scale(t$Verbal.Fluency))
cd$Spelling <- as.numeric(scale(t$Spelling))
cd$T.SWE.WPS <- as.numeric(scale(t$T.SWE.WPS))
cd$T.PDE.WPS <- as.numeric(scale(t$T.PDE.WPS))
cd$PIAT <- as.numeric(scale(t$PIAT))
cd$Vocab <- as.numeric(scale(t$Vocab))
cd$DPE.Phon <- as.numeric(scale(t$DPE.Phon))
cd$DPE.Pred <- as.numeric(scale(t$DPE.Pred))
cd$Pred.effect.GZD <- as.numeric(scale(t$Pred.effect.GZD))
cd$Mean.GZD <- as.numeric(scale(t$Mean.GZD))

# use subject ID as a rowname to make sure that you can correctly match each sub to its cluster
rownames(cd)<-cd$sub

# remove participants with missing values and remove variables that are not of interest
cd[!complete.cases(cd),]
clusterdata<-cd[c(2:14,18,20:30),c(3:5,7:8,10:11)]
sub <- c(2:14,18,20:30)
```

Doing so reveals the dendogram with the 2- and 3-cluster solutions outlined in blue and red respectively

```{r cluster analysis dendogram, echo = TRUE, message=FALSE, warning=FALSE}
# Ward Hierarchical Clustering
d <- dist(clusterdata, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward.D2") # ward.D2 is actualy ward's method

plot(fit) # display dendogram
groups2 <- cutree(fit, k=2) # cut tree into 2 clusters
groups3 <- cutree(fit, k=3) # cut tree into 3 clusters
groups4 <- cutree(fit, k=4) # cut tree into 4 clusters
groups5 <- cutree(fit, k=5) # cut tree into 5 clusters
groups6 <- cutree(fit, k=6) # cut tree into 6 clusters
# draw dendogram with borders around the k clusters 
rect.hclust(fit, k=3, border="red")
rect.hclust(fit, k=2, border="blue")
```

Next, we can use the clValid package to compare the different clustering methods and determine which cluster solution is best. When doing so, the aim is to minimize connectivity, maximize silhouette (max value 1), and maximize the Dunn index (max value infinity). 

Doing so reveals that the hierarchical clustering method was the best (providing a better solution than kmeans or pam), but depending on the validation measure, either the 2, 3, or 4 cluster solution was best. Going forward, we investigate the 2- and 3-cluster solutions.

```{r cluster analysis validation, echo = TRUE, message=FALSE, warning=FALSE}
#######################################
##  determing best cluster solution  ##
#######################################

library(clValid)

clmethods <- c("hierarchical","kmeans","pam")
v <- clValid(clusterdata, nClust = 2:6,
             clMethods = clmethods, validation = "internal") #comparing methods and cluster solutions for k= 2-6 solutions

# Connectivity should be minimized (minimum value 0), silhouette should be maximized (max value 1), Dunn index should be maximized (max value infinity)
summary(v) #summary of best method (hierarchical, kmeans, pam) and number of clusters
plot(v) # plots for internal validataion measures (connectivity, Dunn, Silhouette)
```

Here we can see the summary statistics for various reading time and language skill assessments for the different clusters in the 2- and 3-cluster solutions.

```{r cluster summary stats, echo = TRUE, message=FALSE, warning=FALSE}
#merge assessment data with cluster data grouping variables
groups<-data.frame(groups2,groups3,sub)
full<-merge(cd,groups,by="sub")
full$groups2 <- as.factor(full$groups2)
full$groups3 <- as.factor(full$groups3)

stats2<-describeBy(full, full$groups2, skew=FALSE, ranges=FALSE)
stats2

stats3<-describeBy(full, full$groups3, skew=FALSE, ranges=FALSE)
stats3
```

Next we can take a look at the associated figures and group comparisons, to see what type of reader profiles the cluster analysis revealed. First, we consider the 2-cluster solution.

When breaking our participants into 2 clusters, the analysis reveals a generally higher-skilled (cluster 1) and generally-lower skilled (cluster 2) group of readers. Cluster 1 scored significantly higher on spelling and rapid non-word naming, and numerically higher on vocabulary and reading comprehension (differences between rapid word naming and mean gaze durations were very small). Cluster 1 also had a significantly earlier mean divergence point (they use sound information more rapidly during reading), but had a significantly smaller predictability effect (they seem to be less reliant on context to identify words). The first evidence that we might have identified a dissociation between phonological and predictive reading strategies (and evidence that relying on phonological processing might be indicative of generally more highly skilled readers).

```{r 2 cluster solution, echo = FALSE, message=FALSE, warning=FALSE}
# 2 Clusters Figures and significance tests
ggplot(full, aes(x=groups2, y=Spelling, colour=groups2)) + geom_boxplot() + geom_point() +
  labs(title = "Spelling", x = "", y = "WJ Spelling z-score", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("skyblue3","palegreen3"))

test <- lm(data=full, Spelling ~ groups2)
summary(test)

ggplot(full, aes(x=groups2, y=Vocab, colour=groups2)) + geom_boxplot() + geom_point() +
  labs(title = "Vocabulary", x = "", y = "WASI-II z-score", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("skyblue3","palegreen3"))

test <- lm(data=full, Vocab ~ groups2)
summary(test)
  
ggplot(full, aes(x=groups2, y=PIAT, colour=groups2)) + geom_boxplot() + geom_point() +
  labs(title = "Reading Comprehension", x = "", y = "PIAT-R z-score", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("skyblue3","palegreen3"))

test <- lm(data=full, PIAT ~ groups2)
summary(test)

ggplot(full, aes(x=groups2, y=T.PDE.WPS, colour=groups2)) + geom_boxplot() + geom_point() +
  labs(title = "Rapid Non-Word Naming", x = "", y = "TOWRE PDE z-score", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("skyblue3","palegreen3"))

test <- lm(data=full, T.PDE.WPS ~ groups2)
summary(test)

ggplot(full, aes(x=groups2, y=T.SWE.WPS, colour=groups2)) + geom_boxplot() + geom_point() +
  labs(title = "Rapid Word Naming", x = "", y = "TOWRE SWE z-score", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("skyblue3","palegreen3"))

test <- lm(data=full, T.SWE.WPS ~ groups2)
summary(test)

ggplot(full, aes(x=groups2, y=Mean.GZD, colour=groups2)) + geom_boxplot() + geom_point() +
  labs(title = "Gaze Duration", x = "", y = "Gaze Duration (ms)", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("skyblue3","palegreen3"))

test <- lm(data=full, Mean.GZD ~ groups2)
summary(test)

ggplot(full, aes(x=groups2, y=DPE.Phon, colour=groups2)) + geom_boxplot() + geom_point() +
  labs(title = "Divergence Point (Phonological Processing)", x = "", y = "Divergence Point Estimate (ms)", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("skyblue3","palegreen3"))

test <- lm(data=full, DPE.Phon ~ groups2)
summary(test)

ggplot(full, aes(x=groups2, y=Pred.effect.GZD, colour=groups2)) + geom_boxplot() + geom_point() +
  labs(title = "Size of Predictability Effect", x = "", y = "Size of Predictability Effect (ms)", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("skyblue3","palegreen3"))

test <- lm(data=full, Pred.effect.GZD ~ groups2)
summary(test)
```

Finally, we consider the 3-cluster solution.

When breaking our participants into 3 clusters, the analysis provides additional evidence in support of various reading strategies, particularly it identifies one group of readers (cluster 3) who show a large effect of predictability and a very late effect of phonology (i.e., they seem to rely heavily on prior context to identify words) and another group of readers (cluster 2) who shows the opposite pattern--early use of phonology (i.e., early divergence point estimates) but a very small effect of predictability. The third cluster (cluster 1) falls in the middle and seems to be using a mixture of these two strategies (or at least contains a mixture of participants who use each strategy).

Compared to Cluster 2 (our readers who seem to rely more on phonological coding), Cluster 3 (our predictive strategy readers) had significantly later divergence point estimates and significantly larger effects of predictability. They also showed some interesting differences in underlying language skills: Although they did not differ from Cluster 2 on measures of vocabulary or reading comprehension, they had significantly lower scores on spelling and rapid non-word naming (though their scores on rapid word naming did not differ--suggesting that the difference is driven by poorer skills extracting phonology from written information, rather than slower processing speed). Finally, there was a tendency for readers relying on predictive strategies to have marginally slower mean gaze durations during reading, suggesting slightly slower word identification speeds.


```{r 3 cluster solution, echo = FALSE, message=FALSE, warning=FALSE}
# 3 Clusters Figures
ggplot(full, aes(x=groups3, y=DPE.Phon, colour=groups3)) + geom_boxplot() + geom_point() +
  labs(title = "Divergence Point (Phonological Processing)", x = "", y = "Divergence Point Estimate (ms)", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("orchid4","skyblue3","palegreen3"))

test <- lm(data=full, DPE.Phon ~ groups3)
summary(test)
fu.test <- lm(data=subset(full,full$groups3 != 1), DPE.Phon ~ groups3)
summary(fu.test)

ggplot(full, aes(x=groups3, y=Pred.effect.GZD, colour=groups3)) + geom_boxplot() + geom_point() +
  labs(title = "Size of Predictability Effect", x = "", y = "Size of Predictability Effect (ms)", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("orchid4","skyblue3","palegreen3"))

test <- lm(data=full, Pred.effect.GZD ~ groups3)
summary(test)
fu.test <- lm(data=subset(full,full$groups3 != 1), Pred.effect.GZD ~ groups3)
summary(fu.test)

ggplot(full, aes(x=groups3, y=Vocab, colour=groups3)) + geom_boxplot() + geom_point() +
  labs(title = "Vocabulary", x = "", y = "WASI-II z-score", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("orchid4","skyblue3","palegreen3"))

test <- lm(data=full, Vocab ~ groups3)
summary(test)
fu.test <- lm(data=subset(full,full$groups3 != 1), Vocab ~ groups3)
summary(fu.test)

ggplot(full, aes(x=groups3, y=PIAT, colour=groups3)) + geom_boxplot() + geom_point() +
  labs(title = "Reading Comprehension", x = "", y = "PIAT-R z-score", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("orchid4","skyblue3","palegreen3"))

test <- lm(data=full, PIAT ~ groups3)
summary(test)
fu.test <- lm(data=subset(full,full$groups3 != 1), PIAT ~ groups3)
summary(fu.test)
  
ggplot(full, aes(x=groups3, y=Spelling, colour=groups3)) + geom_boxplot() + geom_point() +
  labs(title = "Spelling", x = "", y = "WJ Spelling z-score", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("orchid4","skyblue3","palegreen3"))

test <- lm(data=full, Spelling ~ groups3)
summary(test)
fu.test <- lm(data=subset(full,full$groups3 != 1), Spelling ~ groups3)
summary(fu.test)

ggplot(full, aes(x=groups3, y=T.PDE.WPS, colour=groups3)) + geom_boxplot() + geom_point() +
  labs(title = "Rapid Non-Word Naming", x = "", y = "TOWRE PDE z-score", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("orchid4","skyblue3","palegreen3"))

test <- lm(data=full, T.PDE.WPS ~ groups3)
summary(test)
fu.test <- lm(data=subset(full,full$groups3 != 1), T.PDE.WPS ~ groups3)
summary(fu.test)

ggplot(full, aes(x=groups3, y=T.SWE.WPS, colour=groups3)) + geom_boxplot() + geom_point() +
  labs(title = "Rapid Word Naming", x = "", y = "TOWRE SWE z-score", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("orchid4","skyblue3","palegreen3"))

test <- lm(data=full, T.SWE.WPS ~ groups3)
summary(test)
fu.test <- lm(data=subset(full,full$groups3 != 1), T.SWE.WPS ~ groups3)
summary(fu.test)

ggplot(full, aes(x=groups3, y=Mean.GZD, colour=groups3)) + geom_boxplot() + geom_point() +
  labs(title = "Gaze Duration", x = "", y = "Gaze Duration (ms)", colour = "Clusters") + theme_grey(base_size=18) +
  theme(axis.text.x = element_text(colour="white", size= 0), axis.text.y = element_text(colour = "grey4", size=16)) +
  scale_color_manual(values=c("orchid4","skyblue3","palegreen3"))

test <- lm(data=full, Mean.GZD ~ groups3)
summary(test)
fu.test <- lm(data=subset(full,full$groups3 != 1), Mean.GZD ~ groups3)
summary(fu.test)

```
